{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning with scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "\n",
    "* <a href=#Clust>Clustering for dataset exploration</a>\n",
    "* <a href=#Viz>Visualization with hierarchical clustering and t-SNE</a>\n",
    "* <a href=#Decorr>Decorrelating your data and dimension reduction</a>\n",
    "* <a href=#Disc>Discovering interpretable features</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages and Set Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, normalize\n",
    "from sklearn.manifold import TSN\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All embeddings and clusterings can be saved and loaded into this script. Be carful with overwriting cluster caches as soon as cell type annotation has started as cluster labels may be shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set whether anndata objects are recomputed or loaded from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_recomp = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set whether clustering is recomputed or loaded from saved .obs file. Loading makes sense if the clustering changes due to a change in scanpy or one of its dependencies and the number of clusters or the cluster labels change accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_recluster = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set whether cluster cache is overwritten. Note that the cache exists for reproducibility of clustering, see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_write_cluster_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set whether to produce plots, set to False for test runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set whether observations should be calculated. If false, it is necessary to read cacheed file that contains the necssary information. It then shows the the distributions of counts and genes, as well as mt_frac after filtering. \n",
    "Set to true in order to see the data before filtering and follow the decisions for cutoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_create_observations = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Dataloading\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering for dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering 2D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:   \n",
    "   # Create a KMeans instance with 3 clusters: model\n",
    "    model = KMeans(n_clusters=3)\n",
    "\n",
    "    # Fit model to points\n",
    "    model.fit(points)\n",
    "\n",
    "    # Determine the cluster labels of new_points: labels\n",
    "    labels = model.predict(new_points)\n",
    "\n",
    "    # Print cluster labels of new_points\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect your clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True: \n",
    "    # Assign the columns of new_points: xs and ys\n",
    "    xs = new_points[:,0]\n",
    "    ys = new_points[:,1]\n",
    "\n",
    "    # Make a scatter plot of xs and ys, using labels to define the colors\n",
    "    plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
    "\n",
    "    # Assign the cluster centers: centroids\n",
    "    centroids = model.cluster_centers_\n",
    "\n",
    "    # Assign the columns of centroids: centroids_x, centroids_y\n",
    "    centroids_x = centroids[:,0]\n",
    "    centroids_y = centroids[:,1]\n",
    "\n",
    "    # Make a scatter plot of centroids_x and centroids_y\n",
    "    plt.scatter(centroids_x, centroids_y, marker='D', s=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clusters of grain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True: \n",
    "    ks = range(1, 6)\n",
    "    inertias = []\n",
    "\n",
    "    for k in ks:\n",
    "        # Create a KMeans instance with k clusters: model\n",
    "        model =KMeans(n_clusters=k)\n",
    "\n",
    "        # Fit model to samples\n",
    "        model.fit(samples)\n",
    "\n",
    "        # Append the inertia to the list of inertias\n",
    "        inertias.append(model.inertia_)\n",
    "\n",
    "    # Plot ks vs inertias\n",
    "    plt.plot(ks, inertias, '-o')\n",
    "    plt.xlabel('number of clusters, k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.xticks(ks)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the grain clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True: \n",
    "    # Create a KMeans model with 3 clusters: model\n",
    "    model = KMeans(n_clusters=3)\n",
    "\n",
    "    # Use fit_predict to fit model and obtain cluster labels: labels\n",
    "    labels = model.fit_predict(samples)\n",
    "\n",
    "    # Create a DataFrame with labels and varieties as columns: df\n",
    "    df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "    # Create crosstab: ct\n",
    "    ct = pd.crosstab(df['labels'],df['varieties'])\n",
    "\n",
    "    # Display ct\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling fish data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True: \n",
    "    # Create scaler: scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Create KMeans instance: kmeans\n",
    "    kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "    # Create pipeline: pipeline\n",
    "    pipeline = make_pipeline(scaler, kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering the fish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True: \n",
    "    # Fit the pipeline to samples\n",
    "    pipeline.fit(samples)\n",
    "\n",
    "    # Calculate the cluster labels: labels\n",
    "    labels = pipeline.predict(samples)\n",
    "\n",
    "    # Create a DataFrame with labels and species as columns: df\n",
    "    df = pd.DataFrame({'labels': labels, 'species': species})\n",
    "\n",
    "    # Create crosstab: ct\n",
    "    ct = pd.crosstab(df['labels'],df['species'])\n",
    "\n",
    "    # Display ct\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering stocks using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create a normalizer: normalizer\n",
    "    normalizer = Normalizer()\n",
    "\n",
    "    # Create a KMeans model with 10 clusters: kmeans\n",
    "    kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "    # Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "    pipeline = make_pipeline(normalizer, kmeans)\n",
    "\n",
    "    # Fit pipeline to the daily price movements\n",
    "    pipeline.fit(movements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which stocks move together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Predict the cluster labels: labels\n",
    "    labels = pipeline.predict(movements)\n",
    "\n",
    "    # Create a DataFrame aligning labels and companies: df\n",
    "    df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
    "\n",
    "    # Display df sorted by cluster label\n",
    "    print(df.sort_values(by='labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with hierarchical clustering and t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering of the grain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Calculate the linkage: mergings\n",
    "    mergings = linkage(samples,method='complete')\n",
    "\n",
    "    # Plot the dendrogram, using varieties as labels\n",
    "    dendrogram(mergings,\n",
    "               labels=varieties,\n",
    "               leaf_rotation=90,\n",
    "               leaf_font_size=6,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchies of stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Normalize the movements: normalized_movements\n",
    "    normalized_movements = normalize(movements)\n",
    "\n",
    "    # Calculate the linkage: mergings\n",
    "    mergings = linkage(normalized_movements, method='complete')\n",
    "\n",
    "    # Plot the dendrogram\n",
    "    dendrogram(mergings,\n",
    "               labels=companies,\n",
    "               leaf_rotation=90,\n",
    "               leaf_font_size=6\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different linkage, different hierarchical clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Calculate the linkage: mergings\n",
    "    mergings = linkage(samples, method='single')\n",
    "\n",
    "    # Plot the dendrogram\n",
    "    dendrogram(mergings,\n",
    "               labels=country_names,\n",
    "               leaf_rotation=90,\n",
    "               leaf_font_size=6\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Use fcluster to extract labels: labels\n",
    "    labels = fcluster(mergings, 6, criterion='distance')\n",
    "\n",
    "    # Create a DataFrame with labels and varieties as columns: df\n",
    "    df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "    # Create crosstab: ct\n",
    "    ct = pd.crosstab(df['labels'],df['varieties'])\n",
    "\n",
    "    # Display ct\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE visualization of grain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create a TSNE instance: model\n",
    "    model = TSNE(learning_rate=200)\n",
    "\n",
    "    # Apply fit_transform to samples: tsne_features\n",
    "    tsne_features = model.fit_transform(samples)\n",
    "\n",
    "    # Select the 0th feature: xs\n",
    "    xs = tsne_features[:,0]\n",
    "\n",
    "    # Select the 1st feature: ys\n",
    "    ys = tsne_features[:,1]\n",
    "\n",
    "    # Scatter plot, coloring by variety_numbers\n",
    "    plt.scatter(xs, ys, c=variety_numbers)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A t-SNE map of the stock market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create a TSNE instance: model\n",
    "    model = TSNE(learning_rate=50)\n",
    "\n",
    "    # Apply fit_transform to normalized_movements: tsne_features\n",
    "    tsne_features = model.fit_transform(normalized_movements)\n",
    "\n",
    "    # Select the 0th feature: xs\n",
    "    xs = tsne_features[:,0]\n",
    "\n",
    "    # Select the 1th feature: ys\n",
    "    ys = tsne_features[:,1]\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.scatter(xs, ys, alpha=0.5)\n",
    "\n",
    "    # Annotate the points\n",
    "    for x, y, company in zip(xs, ys, companies):\n",
    "        plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorrelating your data and dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlated data in nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Assign the 0th column of grains: width\n",
    "    width = grains[:,0]\n",
    "\n",
    "    # Assign the 1st column of grains: length\n",
    "    length = grains[:,1]\n",
    "\n",
    "    # Scatter plot width vs length\n",
    "    plt.scatter(width, length)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the Pearson correlation\n",
    "    correlation, pvalue = pearsonr(width, length)\n",
    "\n",
    "    # Display the correlation\n",
    "    print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decorrelating the grain measurements with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create PCA instance: model\n",
    "    model = PCA()\n",
    "\n",
    "    # Apply the fit_transform method of model to grains: pca_features\n",
    "    pca_features = model.fit_transform(grains)\n",
    "\n",
    "    # Assign 0th column of pca_features: xs\n",
    "    xs = pca_features[:,0]\n",
    "\n",
    "    # Assign 1st column of pca_features: ys\n",
    "    ys = pca_features[:,1]\n",
    "\n",
    "    # Scatter plot xs vs ys\n",
    "    plt.scatter(xs, ys)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the Pearson correlation of xs and ys\n",
    "    correlation, pvalue = pearsonr(xs, ys)\n",
    "\n",
    "    # Display the correlation\n",
    "    print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Make a scatter plot of the untransformed points\n",
    "    plt.scatter(grains[:,0], grains[:,1])\n",
    "\n",
    "    # Create a PCA instance: model\n",
    "    model = PCA()\n",
    "\n",
    "    # Fit model to points\n",
    "    model.fit(grains)\n",
    "\n",
    "    # Get the mean of the grain samples: mean\n",
    "    mean = model.mean_\n",
    "\n",
    "    # Get the first principal component: first_pc\n",
    "    first_pc = model.components_[0,:]\n",
    "\n",
    "    # Plot first_pc as an arrow, starting at mean\n",
    "    plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
    "\n",
    "    # Keep axes on same scale\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance of the PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create scaler: scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Create a PCA instance: pca\n",
    "    pca = PCA()\n",
    "\n",
    "    # Create pipeline: pipeline\n",
    "    pipeline =make_pipeline(scaler, pca)\n",
    "\n",
    "    # Fit the pipeline to 'samples'\n",
    "    pipeline.fit(samples)\n",
    "\n",
    "    # Plot the explained variances\n",
    "    features = range(pca.n_components_)\n",
    "    plt.bar(features, pca.explained_variance_)\n",
    "    plt.xlabel('PCA feature')\n",
    "    plt.ylabel('variance')\n",
    "    plt.xticks(features)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension reduction of the fish measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create a PCA model with 2 components: pca\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    # Fit the PCA instance to the scaled samples\n",
    "    pca.fit(scaled_samples)\n",
    "\n",
    "    # Transform the scaled samples: pca_features\n",
    "    pca_features = pca.transform(scaled_samples)\n",
    "\n",
    "    # Print the shape of pca_features\n",
    "    print(pca_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tf-idf word-frequency array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create a TfidfVectorizer: tfidf\n",
    "    tfidf = TfidfVectorizer()\n",
    "\n",
    "    # Apply fit_transform to document: csr_mat\n",
    "    csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "    # Print result of toarray() method\n",
    "    print(csr_mat.toarray())\n",
    "\n",
    "    # Get the words: words\n",
    "    words = tfidf.get_feature_names()\n",
    "\n",
    "    # Print words\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering Wikipedia part I - Create a Pipeline object consisting of a TruncatedSVD followed by KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create a TruncatedSVD instance: svd\n",
    "    svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "    # Create a KMeans instance: kmeans\n",
    "    kmeans = KMeans(n_clusters=6)\n",
    "\n",
    "    # Create a pipeline: pipeline\n",
    "    pipeline = make_pipeline(svd, kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering Wikipedia part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Fit the pipeline to articles\n",
    "    pipeline.fit(articles)\n",
    "\n",
    "    # Calculate the cluster labels: labels\n",
    "    labels = pipeline.predict(articles)\n",
    "\n",
    "    # Create a DataFrame aligning labels and titles: df\n",
    "    df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "    # Display df sorted by cluster label\n",
    "    print(df.sort_values(by='label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering interpretable features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF applied to Wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create an NMF instance: model\n",
    "    model = NMF(n_components=6)\n",
    "\n",
    "    # Fit the model to articles\n",
    "    model.fit(articles)\n",
    "\n",
    "    # Transform the articles: nmf_features\n",
    "    nmf_features = model.transform(articles)\n",
    "\n",
    "    # Print the NMF features\n",
    "    print(nmf_features.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF features of the Wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create a pandas DataFrame: df\n",
    "    df = pd.DataFrame(nmf_features, index=titles)\n",
    "\n",
    "    # Print the row for 'Anne Hathaway'\n",
    "    print(df.loc['Anne Hathaway'])\n",
    "\n",
    "    # Print the row for 'Denzel Washington'\n",
    "    print(df.loc['Denzel Washington'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF learns topics of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bool_recomp == True:\n",
    "    # Create a DataFrame: components_df\n",
    "    components_df = pd.DataFrame(model.components_, columns=words)\n",
    "\n",
    "    # Print the shape of the DataFrame\n",
    "    print(components_df.shape)\n",
    "\n",
    "    # Select row 3: component\n",
    "    component = components_df.iloc[3,:]\n",
    "\n",
    "    # Print result of nlargest\n",
    "    print(component.nlargest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
